{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Neural Network in 11 Lines\n",
    "\n",
    "If you’ve been paying attention to developments in technology, you’ve probably heard the words machine learning. This field has risen to prominence due to companies like Google and Facebook applying machine-learning solutions to solve some of their more challenging problems. The solutions have been so effective that leading researches like Andrew Ng, a Stanford professor, have gone as far as to call machine learning the “next electricity.” A key building block in these solutions is something called a Neural Network. \n",
    "\n",
    "Neural Networks have made it possible for SpaceX to recover and reuse rocket boosters. They’ve defeated the human champion in Go. They’ve also made it possible for you to use your voice to instruct your phone (and sometimes it even listens!). This notebook will show you how to create your own neural network in python which will learn to mimic the “exclusive or” function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is XOR\n",
    "\n",
    "Before creating the neural network it is important that we understand what we are trying to do. XOR is a function that takes two **boolean** values. A boolean value can be either 0 or 1. The XOR of two boolean values is equal to 1 if exactly one of the boolean values is equal to 1. Otherwise it is 0. Fig 1 shows the output of XOR for any combination of two boolean values.\n",
    "\n",
    "<table style=\"width:25%\">\n",
    "    <tr>\n",
    "        <th>A</th> <th>B</th> <th>A XOR B</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>0</td> <td>0</td> <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>0</td> <td>1</td> <td>1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>1</td> <td>0</td> <td>1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>1</td> <td>1</td> <td>0</td>\n",
    "    </tr>\n",
    "</table>\n",
    "<center style=\"font-size:10px\">Fig 1: Table showing the values of the XOR operation</center>\n",
    "\n",
    "## Why XOR\n",
    "\n",
    "The reason why we’ve chosen XOR is because it is the simplest function that a **linear classifier** can’t represent. A linear classifier simply means our function would be \n",
    "$$f(A,B) = 1  \\text{  if  }  C_1 A + C_2 B > 0$$ \n",
    "$$f(A,B) = 0  \\text{  otherwise }$$\n",
    "<br />\n",
    "where A and B are our inputs and C<sub>1</sub> and C<sub>2</sub> are our weights. No matter what weights you pick, you will never be able create a classifier that mimics XOR. This is linear because our model can be thought of as a linear combination of the model weights and inputs. Because of this the decision boundary between a 1 and 0 is a line. The despair of trying to fit a line to correctly mimic XOR is illustrated in fig 2.\n",
    "<img src=\"./xor1.png\">\n",
    "<center style=\"font-size:10px\">Fig 2: Plot showing the impossiblity of creating a linear classifier that mimics XOR. Blue dots represent input that should be classified as 0 and red represents input that should be classified as 1.</center>\n",
    "<br />\n",
    "In order to create a model that can correctly mimic XOR. We need one capable of a more complex decision boundary. Thats where neural networks come in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do I do it\n",
    "If you aren't familiar with basic idea of a neural network. I highly recommend taking a look at the <a href=\"https://github.com/arjunb98/xor-neural-net/blob/master/TDD%20Final%20Draft.pdf\">technical definition document</a> in this repository to get a better idea of what this model looks like. Once thats done we're ready to jump in.\n",
    "\n",
    "### What are the preqrequisites\n",
    "There are multiple ways to use these instructions so there aren't any hard and fast prerequisites. Even if you aren't very confident in your linear algebra and coding ability there is still value to be gained from these instructions. Namely seeing an implementation of a neural network can give you a rough idea of how they work. In addition, though the definitions and instructions require some knowledge about vector calculus, I also try to give them in lay-terms as well. As long as you have a python interpreter, and a text editor and know how to run your own code you'll be fine. \n",
    "<br /><br />However, to get the most out of this guide to use the instructions would be to fork this repository and look at a copy of this <a href=\"https://jupyter.org/\">jupyter notebook</a>. This requires you to set up jupyter on your computer. In addition, a solid foundation in calculus and linear algebra will maximize the utility of this guide. Some knowledge of the python linear algebra library numpy may also be helpful.\n",
    "\n",
    "### Can I just have the code\n",
    "Sure. Here's a neural network in 11 lines of python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y = np.array([[0,1,1,0]]).T\n",
    "w0 = 2*np.random.random((2,4)) - 1\n",
    "w1 = 2*np.random.random((4,1)) - 1\n",
    "for j in range(60000):\n",
    "    l1 = 1/(1+np.exp(-(np.dot(X,w0))))\n",
    "    l2 = 1/(1+np.exp(-(np.dot(l1,w1))))\n",
    "    l2_update = (y-l2)*(l2*(1-l2))\n",
    "    l1_update = l2_update.dot(w1.T)*(l1*(1-l1))\n",
    "    w1 += l1.T.dot(l2_update)\n",
    "    w0 += X.T.dot(l1_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to verify it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: 0 \tB: 0 \tA XOR B: 0 \tF(A,B): 0\n",
      "A: 0 \tB: 1 \tA XOR B: 1 \tF(A,B): 1\n",
      "A: 1 \tB: 0 \tA XOR B: 1 \tF(A,B): 1\n",
      "A: 1 \tB: 1 \tA XOR B: 0 \tF(A,B): 0\n"
     ]
    }
   ],
   "source": [
    "for i in X:\n",
    "    l1 = 1/(1+np.exp(-(np.dot(i,w0))))\n",
    "    l2 = 1/(1+np.exp(-(np.dot(l1,w1))))\n",
    "    if l2 > 0.5:\n",
    "        ans = 1 \n",
    "    else: \n",
    "        ans = 0 \n",
    "    print('A:', i[0], '\\tB:', i[1], '\\tA XOR B:', i[0]^i[1], '\\tF(A,B):', ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However this is a bit dense and it'll probably be easier to understand if you follow a step by step approach (and uses a few more lines)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions\n",
    "One last thing that may be helpful before jumping into the instructions are definitions of the variables and linear algebra functions that we will be using.\n",
    "<table style=\"width:70%\">\n",
    "    <tr>\n",
    "        <th style=\"text-align:left\">Variable/Operation</th> <th colspan=3 style=\"text-align:left\">Definition</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:left\">X</td> <td colspan=3 style=\"text-align:left\">Input dataset. This a matrix of all possible combinations of two boolean variables</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:left\">y</td> <td colspan=3 style=\"text-align:left\">Output dataset. This is a vector where the ith element corresponds to the XOR of the two values in the ith row of X</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:left\">l0</td> <td colspan=3 style=\"text-align:left\">First layer of the neural network.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:left\">l1</td> <td colspan=3 style=\"text-align:left\">Second layer of the neural network.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:left\">l1</td> <td colspan=3 style=\"text-align:left\">The final layer and output of the neural network.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:left\">w0</td> <td colspan=3 style=\"text-align:left\">First weight matrix of the neural network. This is what connects the input X to the first layer.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:left\">w1</td> <td colspan=3 style=\"text-align:left\">Second weight matrix of the neural network. This is what connects the first layer to the second layer.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:left\">l2_error</td> <td colspan=3 style=\"text-align:left\">This is the amount that the neural network \"missed\". In other words, the objective function.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:left\">l2_delta</td> <td colspan=3 style=\"text-align:left\">This is the error of the network scaled by the confidence(gradient of the error with respect to the output). It's almost identical to the error except that very confident errors are muted. It is also almost the gradient of the w1 matrix but is missing the multiplication with l1.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:left\">l1_error</td> <td colspan=3 style=\"text-align:left\">Weighting l2_delta by the weights in w1 (the derivative of the matrix multiplication between the middle layer and the output), we can calculate the error in the middle/hidden layer.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:left\">l1_delta</td> <td colspan=3 style=\"text-align:left\">This is the l1 error of the network scaled by the confidence (gradient of the error with respect to l1). Again, it's almost identical to the l1_error except that confident errors are muted and is just missing the multiplication with l0 to be the gradient of w0.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:left\">*</td> <td colspan=3 style=\"text-align:left\">Elementwise multiplication, so two vectors of equal size are multiplying corresponding values 1-to-1 to generate a final vector of identical size.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:left\">-</td> <td colspan=3 style=\"text-align:left\">Elementwise subtraction, so two vectors of equal size are subtracting corresponding values 1-to-1 to generate a final vector of identical size.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:left\">x.dot(y)</td> <td colspan=3 style=\"text-align:left\">If x and y are vectors, this is a dot product. If both are matrices, it's a matrix-matrix multiplication. If only one is a matrix, then it's vector matrix multiplication.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:left\">np.exp(x)</td> <td colspan=3 style=\"text-align:left\">The exponential function.</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "**Step 1:** Import numpy. This is a linear algebra library that will allow us to perform the linear algebra operations specified in the definitions sections. We use the alias \"np\" to shorten the length of some of the code. This is a common convention when using this library. \n",
    "<br /> <br />Note: this is our only dependency and we don't need any fancy libaries like tensorflow or pytorch to create a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2:** Define our nonlinear activation function. For this problem we'll use the sigmoid function which is defined as $$\\sigma(x) = \\frac{1}{1+e^{-x}}$$\n",
    "and has a very simple derivative of\n",
    "$$\\sigma'(x) = \\sigma(x)*(1-\\sigma(x))$$\n",
    "<br /><br />\n",
    "Further explanation: The reason we choose the sigmoid function is because it maps any number to a value between 0 and 1. This essentially gives us a probability about our model's prediction. In addition its simple derivative can speed up computation when training our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, deriv=False):\n",
    "    if deriv:\n",
    "        return x*(1-x)\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3:** Define our training dataset. This means creating X and y as specified in the definitions section.\n",
    "<br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    [0,0],\n",
    "    [0,1],\n",
    "    [1,0],\n",
    "    [1,1]\n",
    "])\n",
    "y = np.array([[0],[1],[1],[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4:** Initialize the weights in our network. Be careful to make sure the dimensions of the weight matrices line up.\n",
    "<br /><br />Syntax: np.random.random((a,b)) creates an a x b matrix with random values between 0 and 1. \n",
    "<br /><br />Further explanation: We multiply by 2 and subtract by 1 to give them an initialize mean of 0 which will speed up training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = 2*np.random.random((2,4)) - 1\n",
    "w1 = 2*np.random.random((4,1)) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5:** Define the prediction process for our model. Our final prediction will be l2 but we'll need the intermediate values to train the network.\n",
    "<br /> <br />\n",
    "Explanation: As outlined in the <a href=\"https://github.com/arjunb98/xor-neural-net/blob/master/TDD%20Final%20Draft.pdf\">technical definition document</a>, the first layer and input to our network is \n",
    "$$l_0 = X$$ \n",
    "The next layer, l<sub>1</sub> is \n",
    "$$\\sigma(W_0 l_0) = \\sigma(W_0 X)$$\n",
    "and the second layer l<sub>2</sub> is\n",
    "$$\\sigma(W_1 l_1) = \\sigma(W_1\\sigma(W_0 X))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w0, w1, X):\n",
    "    l0 = X\n",
    "    l1 = sigmoid(np.dot(l0,w0))\n",
    "    l2 = sigmoid(np.dot(l1,w1))\n",
    "    return l0, l1, l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 6:** Define the error metric/objective function that we will use to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(y, l2):\n",
    "    return y - l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 7**: Define the update rule for our model using the gradient of the error with respect to each of our weight matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(l2_error, l2, l1, l0, w1, w0):\n",
    "    l2_delta = l2_error*sigmoid(l2, deriv=True)\n",
    "    l1_error = l2_delta.dot(w1.T)\n",
    "    l1_delta = l1_error * sigmoid(l1, deriv=True)\n",
    "    new_w1 = w1 + l1.T.dot(l2_delta)\n",
    "    new_w0 = w0 + l0.T.dot(l1_delta)\n",
    "    return new_w0, new_w1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 8:** Put it all together and train the model. As we train the model we expect the error to decrease.\n",
    "<br /><br />\n",
    "Note: The number of iterations/epochs that we chose is 60000. This is entirely arbitrary and you are free to experiment with a different number of iterations to see how that affects the model convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:  0.49997696056420715\n",
      "Error:  0.01625877557851503\n",
      "Error:  0.011104514113737637\n",
      "Error:  0.00895348434944655\n",
      "Error:  0.007701762443903242\n",
      "Error:  0.0068590778275089154\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(60000):\n",
    "    l0, l1, l2 = predict(w0, w1, X)\n",
    "    l2_error = error(y, l2)\n",
    "    if iteration % 10000 == 0:\n",
    "        print(\"Error: \", np.mean(np.abs(l2_error)))\n",
    "    w0, w1 = update(l2_error, l2, l1, l0, w1, w0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 9:** Verify it works by comparing the models prediction for each possible combination of 2 boolean values and the actual value of exclusive or.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: 0 \tB: 0 \tA XOR B: 0 \tF(A,B): 0\n",
      "A: 0 \tB: 1 \tA XOR B: 1 \tF(A,B): 1\n",
      "A: 1 \tB: 0 \tA XOR B: 1 \tF(A,B): 1\n",
      "A: 1 \tB: 1 \tA XOR B: 0 \tF(A,B): 0\n"
     ]
    }
   ],
   "source": [
    "for i in X:\n",
    "    l0, l1, l2 = predict(w0, w1, X=i)\n",
    "    if l2 > 0.5:\n",
    "        ans = 1 \n",
    "    else: \n",
    "        ans = 0 \n",
    "    print('A:', i[0], '\\tB:', i[1], '\\tA XOR B:', i[0]^i[1], '\\tF(A,B):', ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where can I go from here\n",
    "Now that you've made a neural network, I'll give you my recommendation and some additional topics that you can look at to learn more about deep learning.\n",
    "\n",
    "\n",
    "#### Recommendation\n",
    "If you're serious about deep learning. Play around with this notebook and then try to recreate an XOR neural net from memory. This may sound silly but being comfortable enough with the linear algebra and calculus behind these models will help a lot when you have to implement a new model architecture from a paper that hasn't been implemented in a popular deep learning library like tensorflow or pytorch.\n",
    "<br /> <br />\n",
    "Beyond this notebook. I highly recommend you take a look at Andrej Karpathy's <a href=\"https://www.youtube.com/watch?v=NfnWJUyUJYU&list=PLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC\">video lectures</a> for CS231n at Stanford as this class takes a very rigourous introduction to deep learning.\n",
    "\n",
    "#### Future work\n",
    "There are quite a few bells and whistles that can be added to this neural network. It can be useful to try implementing them on this toy example before trying to apply them to more complex problems. A few of these in no particular order are:\n",
    "<ul>\n",
    "  <li>Learning rate</li>\n",
    "  <li>Bias</li>\n",
    "  <li>Mini-batches</li>\n",
    "  <li>Momentum</li>\n",
    "  <li>Dropout</li>\n",
    "  <li>Regularization</li>\n",
    "</ul>\n",
    "You can also check out some of python's deep learning libraries like <a href=\"https://www.tensorflow.org/\">tensorflow</a> and <a href=\"https://pytorch.org/\">pytorch</a> which make it easier to assemble some more complex model architectures."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
