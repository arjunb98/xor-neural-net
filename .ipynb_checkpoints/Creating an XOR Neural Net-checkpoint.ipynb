{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Neural Network in 11 Lines\n",
    "\n",
    "If you’ve been paying attention to developments in technology, you’ve probably heard the words machine learning. This field has risen to prominence due to companies like Google and Facebook applying machine-learning solutions to solve some of their more challenging problems. The solutions have been so effective that leading researches like Andrew Ng, a Stanford professor, have gone as far as to call machine learning the “next electricity.” A key building block in these solutions is something called a Neural Network. \n",
    "\n",
    "Neural Networks have made it possible for SpaceX to recover and reuse rocket boosters. They’ve defeated the human champion in Go. They’ve also made it possible for you to use your voice to instruct your phone (and sometimes it even listens!). This notebook will show you how to create your own neural network in python which will learn to mimic the “exclusive or” function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is XOR\n",
    "\n",
    "Before creating the neural network it is important that we understand what we are trying to do. XOR is a function that takes two **boolean** values. A boolean value can be either 0 or 1. The XOR of two boolean values is equal to 1 if exactly one of the boolean values is equal to 1. Otherwise it is 0. Fig 1 shows the output of XOR for any combination of two boolean values.\n",
    "\n",
    "<table style=\"width:25%\">\n",
    "    <tr>\n",
    "        <th>A</th> <th>B</th> <th>A XOR B</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>0</td> <td>0</td> <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>0</td> <td>1</td> <td>1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>1</td> <td>0</td> <td>1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>1</td> <td>1</td> <td>0</td>\n",
    "    </tr>\n",
    "</table>\n",
    "<center style=\"font-size:10px\">Fig 1: Table showing the values of the XOR operation</center>\n",
    "\n",
    "## Why XOR\n",
    "\n",
    "The reason why we’ve chosen XOR is because it is the simplest function that a **linear classifier** can’t represent. A linear classifier simply means our function would be \n",
    "$$f(A,B) = 1  \\text{  if  }  C_1 A + C_2 B > 0$$ \n",
    "$$f(A,B) = 0  \\text{  otherwise }$$\n",
    "<br />\n",
    "where A and B are our inputs and C<sub>1</sub> and C<sub>2</sub> are our weights. No matter what weights you pick, you will never be able create a classifier that mimics XOR. This is linear because our model can be thought of as a linear combination of the model weights and inputs. Because of this the decision boundary between a 1 and 0 is a line. The despair of trying to fit a line to correctly mimic XOR is illustrated in fig 2.\n",
    "<img src=\"./xor1.png\">\n",
    "<center style=\"font-size:10px\">Fig 2: Plot showing the impossiblity of creating a linear classifier that mimics XOR. Blue dots represent input that should be classified as 0 and red represents input that should be classified as 1.</center>\n",
    "<br />\n",
    "In order to create a model that can correctly mimic XOR. We need one capable of a more complex decision boundary. Thats where neural networks come in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do I do it\n",
    "If you aren't familiar with basic idea of a neural network. I highly recommend taking a look at the technical definition document in this repository to get a better idea of what this model looks like. Once thats done we're ready to jump in.\n",
    "\n",
    "### What are the preqrequisites\n",
    "There are multiple ways to use these instructions so there aren't any hard and fast prerequisites. Even if you aren't very confident in your linear algebra and coding ability there is still value to be gained from these instructions. As long as you have a python interpreter, and a text editor and know how to run your own code you'll be fine. \n",
    "<br /><br />However, to get the most out of this guide to use the instructions would be to fork this repository and look at a copy of this <a href=\"https://jupyter.org/\">jupyter notebook</a>. This requires you to set up jupyter on your computer. In addition, a solid foundation in calculus and linear algebra will maximize the utility of this guide. \n",
    "\n",
    "### Can I just have the code\n",
    "Sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y = np.array([[0,1,1,0]]).T\n",
    "w0 = 2*np.random.random((2,4)) - 1\n",
    "w1 = 2*np.random.random((4,1)) - 1\n",
    "for j in range(60000):\n",
    "    l1 = 1/(1+np.exp(-(np.dot(X,w0))))\n",
    "    l2 = 1/(1+np.exp(-(np.dot(l1,w1))))\n",
    "    l2_update = (y-l2)*(l2*(1-l2))\n",
    "    l1_update = l2_update.dot(w1.T)*(l1*(1-l1))\n",
    "    w1 += l1.T.dot(l2_update)\n",
    "    w0 += X.T.dot(l1_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to verify it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: 0 \tB: 0 \tA XOR B: 0 \tF(A,B): 0\n",
      "A: 0 \tB: 1 \tA XOR B: 1 \tF(A,B): 1\n",
      "A: 1 \tB: 0 \tA XOR B: 1 \tF(A,B): 1\n",
      "A: 1 \tB: 1 \tA XOR B: 0 \tF(A,B): 0\n"
     ]
    }
   ],
   "source": [
    "for i in X:\n",
    "    l1 = 1/(1+np.exp(-(np.dot(i,w0))))\n",
    "    l2 = 1/(1+np.exp(-(np.dot(l1,w1))))\n",
    "    if l2 > 0.5:\n",
    "        ans = 1 \n",
    "    else: \n",
    "        ans = 0 \n",
    "    print('A:', i[0], '\\tB:', i[1], '\\tA XOR B:', i[0]^i[1], '\\tF(A,B):', ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However this is a bit dense and it'll probably be easier to understand if you follow a step by step approach (and uses a few more lines)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions\n",
    "One last thing that may be helpful before jumping into the instructions are definitions of the variables and linear algebra functions that we will be using.\n",
    "<table style=\"width:70%\">\n",
    "    <tr>\n",
    "        <th style=\"text-align:left\">Variable/Operation</th> <th colspan=3 style=\"text-align:left\">Definition</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:left\">X</td> <td colspan=3 style=\"text-align:left\">Input dataset. This a matrix of all possible combinations of two boolean variables</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:left\">y</td> <td colspan=3 style=\"text-align:left\">Output dataset. This is a vector where the ith element corresponds to the XOR of the two values in the ith row of X</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:left\">l0</td> <td colspan=3 style=\"text-align:left\">First layer of the neural network.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:left\">l1</td> <td colspan=3 style=\"text-align:left\">Second layer of the neural network.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:left\">w0</td> <td colspan=3 style=\"text-align:left\">First weight matrix of the neural network. This is what connects the input X to the first layer.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:left\">w1</td> <td colspan=3 style=\"text-align:left\">Second weight matrix of the neural network. This is what connects the first layer to the second layer.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:left\">*</td> <td colspan=3 style=\"text-align:left\">Elementwise multiplication, so two vectors of equal size are multiplying corresponding values 1-to-1 to generate a final vector of identical size.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:left\">-</td> <td colspan=3 style=\"text-align:left\">Elementwise subtraction, so two vectors of equal size are subtracting corresponding values 1-to-1 to generate a final vector of identical size.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:left\">x.dot(y)</td> <td colspan=3 style=\"text-align:left\">If x and y are vectors, this is a dot product. If both are matrices, it's a matrix-matrix multiplication. If only one is a matrix, then it's vector matrix multiplication.</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "**Step 1:** Import numpy. This is a linear algebra library that will allow us to perform the linear algebra operations specified in the definitions sections. We use the alias \"np\" to shorten the length of some of the code. This is a common convention when using this library. \n",
    "<br /> <br />Note: this is our only dependency and we don't need any fancy libaries like tensorflow or pytorch to create a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2:** Define our nonlinear activation function. For this problem we'll use the sigmoid function which is defined as $$\\sigma(x) = \\frac{1}{1+e^{-x}}$$\n",
    "and has a very simple derivative of\n",
    "$$\\sigma'(x) = x*(1-x)$$\n",
    "<br /><br />\n",
    "Further explanation: The reason we choose the sigmoid function is because it maps any number to a value between 0 and 1. This essentially gives us a probability about our model's prediction. In addition its simple derivative can speed up computation when training our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, deriv=False):\n",
    "    if deriv:\n",
    "        return x*(1-x)\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3:** Define our training dataset. This means creating X and y as specified in the definitions section.\n",
    "<br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    [0,0],\n",
    "    [0,1],\n",
    "    [1,0],\n",
    "    [1,1]\n",
    "])\n",
    "y = np.array([[0],[1],[1],[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4:** Initialize the weights in our network. Be careful to make sure the dimensions of the weight matrices line up.\n",
    "<br /><br />Note: We multiply by 2 and subtract by 1 to give them an initialize mean of 0 which will speed up training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = 2*np.random.random((2,4)) - 1\n",
    "w1 = 2*np.random.random((4,1)) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5:** Define the prediction process for our model. Our final prediction will be l2 but we'll need the intermediate values to train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w0, w1, X):\n",
    "    l0 = X\n",
    "    l1 = sigmoid(np.dot(l0,w0))\n",
    "    l2 = sigmoid(np.dot(l1,w1))\n",
    "    return l0, l1, l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 6:** Define the error metric/objective function that we will use to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(y, l2):\n",
    "    return y - l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 7**: Define the update rule for our model using the gradient of the error with respect to each of our weight matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(l2_error, l2, l1, l0, w1, w0):\n",
    "    l2_delta = l2_error*sigmoid(l2, deriv=True)\n",
    "    l1_error = l2_delta.dot(w1.T)\n",
    "    l1_delta = l1_error * sigmoid(l1, deriv=True)\n",
    "    new_w1 = w1 + l1.T.dot(l2_delta)\n",
    "    new_w0 = w0 + l0.T.dot(l1_delta)\n",
    "    return new_w0, new_w1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 8:** Put it all together and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:  0.006827370452982401\n",
      "Error:  0.006310104352953422\n",
      "Error:  0.0058945505585454\n",
      "Error:  0.0055512664599278215\n",
      "Error:  0.005261499902694393\n",
      "Error:  0.0050126679175040625\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(60000):\n",
    "    l0, l1, l2 = predict(w0, w1, X)\n",
    "    l2_error = error(y, l2)\n",
    "    if iteration % 10000 == 0:\n",
    "        print(\"Error: \", np.mean(np.abs(l2_error)))\n",
    "    w0, w1 = update(l2_error, l2, l1, l0, w1, w0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 9:** Verify it works! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: 0 \tB: 0 \tA XOR B: 0 \tF(A,B): 0\n",
      "A: 0 \tB: 1 \tA XOR B: 1 \tF(A,B): 1\n",
      "A: 1 \tB: 0 \tA XOR B: 1 \tF(A,B): 1\n",
      "A: 1 \tB: 1 \tA XOR B: 0 \tF(A,B): 0\n"
     ]
    }
   ],
   "source": [
    "for i in X:\n",
    "    l0, l1, l2 = predict(w0, w1, X=i)\n",
    "    if l2 > 0.5:\n",
    "        ans = 1 \n",
    "    else: \n",
    "        ans = 0 \n",
    "    print('A:', i[0], '\\tB:', i[1], '\\tA XOR B:', i[0]^i[1], '\\tF(A,B):', ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
